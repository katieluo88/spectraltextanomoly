#!/bin/bash
#SBATCH -J 1e-4dct                # Job name
#SBATCH -o /home/jdc396/slurm/logs/%j.out                  # Name of stdout output log file (%j expands to jobID)
#SBATCH -e /home/jdc396/slurm/logs/%j.err                  # Name of stderr output log file (%j expands to jobID)
#SBATCH -N 1                                 # Total number of nodes requested
#SBATCH -n 4                                 # Total number of cores requested
#SBATCH --mem=32G                          # Total amount of (real) memory requested (per node)
#SBATCH -t 24:00:00                          # Time limit (hh:mm:ss)
#SBATCH --partition=sun        # Request partition for resource allocation
#SBATCH --gres=gpu:1                        # Specify a list of generic consumable resources (per node)
source ~/anaconda3/etc/profile.d/conda.sh
conda activate spectral2
cd /home/jdc396/spectraltextanomoly
PYTHONPATH=$(pwd) CUDA_LAUNCH_BLOCKING=1 python train.py --wandb \
    --notes 'binning' \
    --do_train \
    --do_eval \
    --eval_test \
    --model 'bert-base-uncased'\
    --classifier 'spectral' \
    --filter 'low' \
    --attack 'a2t' \
    --dataset 'rt' \
    --seed 46 \
    --eval_metric f1 \
    --num_train_epochs 10 \
    --eval_per_epoch 4 \
    --train_batch_size 32 \
    --eval_batch_size 32 \
    --gradient_accumulation_steps 1  \
    --learning_rate 1e-4
